# ğŸ§ª Mini Pipeline IVI â€” Projeto de Arquitetura de Dados

Este projeto simula um pipeline real utilizado por empresas de ciÃªncia de dados, como a IVI Data Science. Ele Ã© focado em ingestÃ£o, transformaÃ§Ã£o e entrega de dados estruturados em tempo real, usando componentes modernos, desacoplados e prontos para rodar em Kubernetes.

---

## ğŸ¯ Objetivo

Demonstrar capacidade arquitetural e tÃ©cnica para:
- Receber arquivos CSV via upload.
- Armazenar os arquivos em object storage (MinIO).
- Detectar novos arquivos via webhook.
- Processar os dados com Pandas de forma desacoplada.
- Enviar os dados tratados diretamente para o Power BI (Streaming Dataset).
- Rodar tudo em ambiente orquestrado com Kubernetes (MicroK8s no lab).

---

## âš™ï¸ Componentes

| ServiÃ§o     | Tecnologia         | FunÃ§Ã£o                                          |
|-------------|--------------------|--------------------------------------------------|
| MinIO       | Object Storage S3  | Armazena arquivos CSV recebidos                 |
| FastAPI     | Webhook Receiver   | Captura eventos do MinIO e aciona o Processor   |
| Processor   | Python + Pandas    | Realiza o ETL, trata dados e envia ao Power BI  |
| Power BI    | Streaming Dataset  | Visualiza dados tratados em tempo real          |
| OrquestraÃ§Ã£o| Kubernetes / Docker Compose | Gerencia deploys e execuÃ§Ã£o local/lab     |

---

## ğŸ§  Arquitetura Infra/Pipeline

```mermaid
graph TD
  A[Usuario] --> B[MinIO Object Storage]
  B -->|Webhook PUT| C[FastAPI Webhook Receiver]
  C -->|Trigger| D[Processor Python + Pandas]

  subgraph Cluster Kubernetes
    B
    C
    D
  end

  D -->|Read via MinIO SDK| B
  D -->|Transforma CSV| E[Memoria DataFrame]
  D -->|POST JSON| F[Power BI Streaming Dataset]
  D -->|Exporta CSV Tratado| G[MinIO Saida]
```

---

## ğŸ“¦ Estrutura de DiretÃ³rios

```
mini-pipeline-ivi/
â”œâ”€â”€ dic/                    # Doc do projeto
â”œâ”€â”€ api/                    # CÃ³digo da API FastAPI (webhook)
â”œâ”€â”€ terraform/              # IaC para k8s, AWS, Azure e OpenStack
â”œâ”€â”€ Dockerfile.api          # Imagem da API FastAPI
â”œâ”€â”€ Dockerfile.processor    # Imagem do Processor
â”œâ”€â”€ Makefile                # Comandos Ãºteis para build e automaÃ§Ãµes
â”œâ”€â”€ .env.dev                # VariÃ¡veis para desenvolvimento
â”œâ”€â”€ .env.homolog            # VariÃ¡veis para homologaÃ§Ã£o
â”œâ”€â”€ .env.prod               # VariÃ¡veis para produÃ§Ã£o
â”œâ”€â”€ README.md               # Este arquivo
```

---

## ğŸš€ Como rodar (modo local)

### PrÃ©-requisitos

- Docker
- Docker Compose
- Power BI com Streaming Dataset configurado (chave de ingestÃ£o)
- `make` instalado (opcional, mas recomendado)
- `mc` (MinIO Client) instalado e configurado
- Terraform instalado

### Passos

1. **Build dos serviÃ§os**
```bash
make build
```

   Ajuste as variÃ¡veis de ambiente nos arquivos `.env.dev`, `.env.homolog` ou
   `.env.prod` conforme necessÃ¡rio.

2. **Subir tudo com Docker Compose**
Escolha o ambiente desejado. Por exemplo:
```bash
make dev       # desenvolvimento
make homolog   # homologaÃ§Ã£o
make prod      # produÃ§Ã£o
```

3. **Ver logs da API**
```bash
make logs
```

4. **Criar bucket e configurar webhook no MinIO**
```bash
mc alias set local http://localhost:9000 minioadmin minioadmin
mc mb local/teste
mc admin config set local notify_webhook:1 endpoint="http://api:8000/webhook/csv"
mc admin service restart local
mc event add local/teste arn:minio:sqs::1:webhook --event put --suffix .csv
```

5. **Enviar CSV manualmente**
```bash
mc cp dados.csv local/teste/
```

6. **Ver relatÃ³rio no Power BI**
> Configure seu dashboard com base no Streaming Dataset correspondente.

## ğŸš€ Deploy com Terraform

Escolha o ambiente desejado (`k8s`, `aws`, `azure` ou `openstack`):
```bash
make deploy ENV=k8s
```
Este comando executa `terraform apply` na pasta `terraform/<ENV>`.

---

## ğŸ”„ Melhorias futuras

- PersistÃªncia de dados tratados (ex: SQLite, PostgreSQL)
- Agendamento de reprocessamento (Celery ou CronJob)
- Upload de CSV via frontend (UI simples com Dropzone.js)
- Fila para desacoplamento total do Processor (Redis, Kafka)
- AutenticaÃ§Ã£o nos endpoints
- Observabilidade com Prometheus + Grafana
- Versionamento de arquivos tratados (via timestamp ou hash)

---

## ğŸ‘¨â€ğŸ’» Autor

Desenvolvido por **Alan Ramalho** como prova de conceito de arquitetura moderna, modular e orientada a eventos para uso real em projetos de ciÃªncia de dados e business intelligence.